"""
æ•°æ®çˆ¬è™«æœåŠ¡æ¨¡å—
è´Ÿè´£ä»å„ç§APIè·å–ä»£å¸æ•°æ®
"""

import requests
import csv
import json
import time
import os
from typing import List, Dict, Optional, Any
from datetime import datetime
from ..models import TokenInfo
from ..utils import safe_float, safe_int, calculate_age_days
from ..utils.data_manager import DataManager
from ..utils.logger import get_logger


class BaseCrawler:
    """åŸºç¡€çˆ¬è™«ç±»"""

    def __init__(self):
        self.logger = get_logger("crawler")
        self.tokens_data: List[Dict[str, Any]] = []
        self.data_manager = DataManager()
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36"
        }
        self.logger.debug("ğŸ”§ BaseCrawler åˆå§‹åŒ–å®Œæˆ")

    def get_data(self, url: str, params: Dict = None, max_retries: int = 3) -> Optional[Dict]:
        """é€šç”¨æ•°æ®è·å–æ–¹æ³•"""
        self.logger.debug(f"ğŸŒ å‘èµ·è¯·æ±‚: {url}")
        if params:
            self.logger.debug(f"ğŸ“‹ è¯·æ±‚å‚æ•°: {params}")
            
        for attempt in range(1, max_retries + 1):
            try:
                start_time = time.time()
                response = requests.get(url, headers=self.headers, params=params, timeout=15)
                request_time = time.time() - start_time
                
                response.raise_for_status()
                data = response.json()
                
                self.logger.info(f"âœ… è¯·æ±‚æˆåŠŸ: {url} (è€—æ—¶: {request_time:.2f}s, çŠ¶æ€: {response.status_code})")
                self.logger.debug(f"ğŸ“Š å“åº”æ•°æ®å¤§å°: {len(response.content)} bytes")
                
                return data
                
            except requests.exceptions.Timeout:
                self.logger.warning(f"â° è¯·æ±‚è¶…æ—¶ ({attempt}/{max_retries}): {url}")
            except requests.exceptions.ConnectionError:
                self.logger.warning(f"ğŸ”Œ è¿æ¥é”™è¯¯ ({attempt}/{max_retries}): {url}")
            except requests.exceptions.HTTPError as e:
                self.logger.error(f"ğŸ“¡ HTTPé”™è¯¯ ({attempt}/{max_retries}): {e.response.status_code} - {url}")
            except Exception as e:
                self.logger.error(f"âŒ è¯·æ±‚å¤±è´¥ ({attempt}/{max_retries}): {e}")
                
            if attempt < max_retries:
                retry_delay = 2**attempt
                self.logger.info(f"â³ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)  # æŒ‡æ•°é€€é¿
            else:
                self.logger.error(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¯·æ±‚å¤±è´¥: {url}")
                return None

    def save_to_csv(self, filename: str = None) -> None:
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        if not self.tokens_data:
            self.logger.warning("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            os.makedirs("data/snapshots", exist_ok=True)
            filename = f"data/snapshots/tokens_{timestamp}.csv"

        # æ”¶é›†æ‰€æœ‰å­—æ®µ
        all_fields = set()
        for token in self.tokens_data:
            all_fields.update(token.keys())

        fieldnames = list(all_fields)
        
        self.logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜æ•°æ®åˆ°CSV: {filename}")
        self.logger.debug(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(self.tokens_data)} æ¡è®°å½•, {len(fieldnames)} ä¸ªå­—æ®µ")

        try:
            start_time = time.time()
            with open(filename, "w", newline="", encoding="utf-8") as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                for token in self.tokens_data:
                    writer.writerow(token)
            
            save_time = time.time() - start_time
            file_size = os.path.getsize(filename) / 1024  # KB
            self.logger.info(f"âœ… CSVä¿å­˜æˆåŠŸ: {filename} (è€—æ—¶: {save_time:.2f}s, å¤§å°: {file_size:.1f}KB)")
            
        except Exception as e:
            self.logger.exception(f"âŒ ä¿å­˜CSVå¤±è´¥: {e}")

    def save_to_json(self, filename: str = None) -> None:
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        if not self.tokens_data:
            self.logger.warning("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        if filename is None:
            filename = f"tokens_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        try:
            filepath = self.data_manager.get_file_path("csv_data", filename)
            self.logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜æ•°æ®åˆ°JSON: {filepath}")
            
            start_time = time.time()
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(self.tokens_data, f, ensure_ascii=False, indent=2)
            
            save_time = time.time() - start_time
            file_size = os.path.getsize(filepath) / 1024  # KB
            self.logger.info(f"âœ… JSONä¿å­˜æˆåŠŸ: {filepath} (è€—æ—¶: {save_time:.2f}s, å¤§å°: {file_size:.1f}KB)")
            
        except Exception as e:
            self.logger.exception(f"âŒ ä¿å­˜JSONå¤±è´¥: {e}")

    def deduplicate_by_mint(self, keep: int = 1000) -> None:
        """æ ¹æ®mintåœ°å€å»é‡"""
        original_count = len(self.tokens_data)
        self.logger.info(f"ğŸ”„ å¼€å§‹å»é‡å¤„ç†: åŸå§‹æ•°æ® {original_count} æ¡ï¼Œç›®æ ‡ä¿ç•™ {keep} æ¡")
        
        seen = set()
        unique = []

        for token in self.tokens_data:
            mint = token.get("mint")
            if mint and mint not in seen:
                seen.add(mint)
                unique.append(token)
                if len(unique) >= keep:
                    break

        removed_count = original_count - len(unique)
        self.tokens_data = unique
        self.logger.info(f"âœ… å»é‡å®Œæˆ: ä¿ç•™ {len(unique)} ä¸ªä»£å¸ï¼Œç§»é™¤ {removed_count} ä¸ªé‡å¤é¡¹")


class PumpFunCrawler(BaseCrawler):
    """Pump.Fun APIçˆ¬è™«"""

    def __init__(self):
        super().__init__()
        self.base_url = "https://frontend-api-v3.pump.fun/coins"
        self.page_size = 50
        self.headers.update({"Referer": "https://pump.fun/"})
        self.logger.info(f"ğŸ”§ PumpFunCrawler åˆå§‹åŒ–å®Œæˆï¼Œé¡µé¢å¤§å°: {self.page_size}")

    def get_page_data(self, offset: int = 0) -> List[Dict]:
        """è·å–å•é¡µæ•°æ®"""
        params = {
            "offset": offset,
            "limit": self.page_size,
            "sort": "market_cap",
            "includeNsfw": "true",
            "order": "DESC",
        }

        page_num = offset // self.page_size + 1
        self.logger.info(f"ğŸ“– æ­£åœ¨è¯·æ±‚ç¬¬ {page_num} é¡µæ•°æ® (offset: {offset})")

        data = self.get_data(self.base_url, params)
        if data and isinstance(data, list):
            self.logger.debug(f"âœ… ç¬¬ {page_num} é¡µè·å–æˆåŠŸ: {len(data)} æ¡æ•°æ®")
            return data
        else:
            self.logger.error(f"âŒ ç¬¬ {page_num} é¡µè¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸: {type(data)}")
            return []

    def crawl_all_pages(self, max_tokens: int = 1000) -> None:
        """çˆ¬å–æ‰€æœ‰é¡µé¢æ•°æ®"""
        self.logger.info(f"ğŸš€ å¼€å§‹çˆ¬å– Pump.Fun ä»£å¸æ•°æ®ï¼Œç›®æ ‡: {max_tokens} ä¸ªä»£å¸")

        total_tokens = 0
        page = 0
        consecutive_failures = 0
        max_failures = 3

        while total_tokens < max_tokens:
            offset = page * self.page_size
            page_data = self.get_page_data(offset)

            if not page_data:
                consecutive_failures += 1
                self.logger.warning(f"âš ï¸ ç¬¬ {page + 1} é¡µæ— æ•°æ®ï¼Œè¿ç»­å¤±è´¥æ¬¡æ•°: {consecutive_failures}")
                
                if consecutive_failures >= max_failures:
                    self.logger.warning(f"âŒ è¿ç»­ {max_failures} é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                    break
                    
                page += 1
                time.sleep(2)  # å¢åŠ ç­‰å¾…æ—¶é—´
                continue
            
            consecutive_failures = 0  # é‡ç½®å¤±è´¥è®¡æ•°

            # é™åˆ¶æ•°é‡
            remaining = max_tokens - total_tokens
            if len(page_data) > remaining:
                page_data = page_data[:remaining]
                self.logger.debug(f"ğŸ“ æ•°é‡é™åˆ¶: åªå–å‰ {remaining} æ¡æ•°æ®")

            self.tokens_data.extend(page_data)
            total_tokens += len(page_data)

            self.logger.info(f"ğŸ“„ ç¬¬ {page + 1} é¡µ: è·å¾— {len(page_data)} æ¡æ•°æ®ï¼Œç´¯è®¡ {total_tokens} æ¡")

            if total_tokens >= max_tokens:
                self.logger.info(f"ğŸ¯ å·²è¾¾åˆ°ç›®æ ‡æ•°é‡ {max_tokens}ï¼Œåœæ­¢çˆ¬å–")
                break

            page += 1
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

        self.logger.info(f"âœ… çˆ¬å–å®Œæˆï¼Œå…±è·å¾— {len(self.tokens_data)} æ¡ä»£å¸æ•°æ®")

    def to_token_info_list(self) -> List[TokenInfo]:
        """è½¬æ¢ä¸ºTokenInfoå¯¹è±¡åˆ—è¡¨"""
        self.logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢ {len(self.tokens_data)} æ¡æ•°æ®ä¸ºTokenInfoå¯¹è±¡")
        
        tokens = []
        failed_count = 0
        
        for i, data in enumerate(self.tokens_data):
            try:
                token = TokenInfo(
                    mint=data.get("mint", ""),
                    name=data.get("name", ""),
                    symbol=data.get("symbol", ""),
                    usd_market_cap=safe_float(data.get("usd_market_cap", 0)),
                    created_timestamp=safe_int(data.get("created_timestamp", 0)),
                    age_days=calculate_age_days(data.get("created_timestamp", 0)),
                )
                tokens.append(token)
                
                if (i + 1) % 100 == 0:
                    self.logger.debug(f"ğŸ”„ å·²è½¬æ¢ {i + 1}/{len(self.tokens_data)} æ¡æ•°æ®")
                    
            except Exception as e:
                failed_count += 1
                self.logger.error(f"âŒ è½¬æ¢TokenInfoå¤±è´¥ (ç¬¬{i+1}æ¡): {e}")
                continue

        self.logger.info(f"âœ… è½¬æ¢å®Œæˆ: æˆåŠŸ {len(tokens)} æ¡ï¼Œå¤±è´¥ {failed_count} æ¡")
        return tokens


class OKXCrawler(BaseCrawler):
    """OKX APIçˆ¬è™«"""

    def __init__(self):
        super().__init__()
        self.base_url = "https://www.okx.com/api/v5"

    def get_token_holders(self, token_address: str, limit: int = 100) -> List[Dict]:
        """è·å–ä»£å¸æŒæœ‰è€…ä¿¡æ¯"""
        url = f"{self.base_url}/explorer/token/token-holder-list"
        params = {
            "chainId": "501",  # Solana
            "tokenContractAddress": token_address,
            "limit": limit,
            "page": "1",
        }

        data = self.get_data(url, params)
        if data and data.get("code") == "0":
            return data.get("data", [])
        return []

    def get_address_tokens(self, address: str) -> List[Dict]:
        """è·å–åœ°å€æŒæœ‰çš„ä»£å¸åˆ—è¡¨"""
        url = f"{self.base_url}/explorer/address/token-balance"
        params = {"chainId": "501", "address": address}

        data = self.get_data(url, params)
        if data and data.get("code") == "0":
            return data.get("data", [])
        return []


class CrawlerFactory:
    """çˆ¬è™«å·¥å‚ç±»"""

    @staticmethod
    def create_crawler(crawler_type: str) -> BaseCrawler:
        """åˆ›å»ºçˆ¬è™«å®ä¾‹"""
        if crawler_type.lower() == "pumpfun":
            return PumpFunCrawler()
        elif crawler_type.lower() == "okx":
            return OKXCrawler()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„çˆ¬è™«ç±»å‹: {crawler_type}")


# å‘åå…¼å®¹çš„ç±»åˆ«å
PumpFunAPICrawler = PumpFunCrawler
