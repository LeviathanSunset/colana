"""
æ•°æ®æ–‡ä»¶ç®¡ç†æ¨¡å—
ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ•°æ®å’Œæ—¥å¿—æ–‡ä»¶ï¼ŒåŒ…æ‹¬è‡ªåŠ¨æ¸…ç†åŠŸèƒ½
"""

import os
import json
import shutil
from datetime import datetime
from pathlib import Path
from typing import List, Dict


class DataManager:
    """æ•°æ®æ–‡ä»¶ç®¡ç†å™¨"""
    
    def __init__(self, base_dir: str = "storage"):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        
        Args:
            base_dir: ç»Ÿä¸€å­˜å‚¨ç›®å½•åç§°
        """
        self.base_dir = Path(base_dir)
        self.max_files = 50  # æœ€å¤§æ–‡ä»¶æ•°é‡
        
        # æ–‡ä»¶ç±»å‹é…ç½®
        self.file_types = {
            "analysis": {
                "pattern": "analysis_*.json",
                "description": "ä»£å¸åˆ†æç»“æœ",
                "max_files": 20
            },
            "holders": {
                "pattern": "holders_raw_*.json", 
                "description": "æŒæœ‰è€…åŸå§‹æ•°æ®",
                "max_files": 15
            },
            "jupiter": {
                "pattern": "jupiter_tokens_*.json",
                "description": "Jupiterä»£å¸æ•°æ®", 
                "max_files": 10
            },
            "logs": {
                "pattern": "okx_crawler_*.log",
                "description": "OKXçˆ¬è™«æ—¥å¿—",
                "max_files": 5
            },
            "csv_data": {
                "pattern": "*.csv",
                "description": "CSVæ•°æ®æ–‡ä»¶",
                "max_files": 2  # åªä¿ç•™now.csvå’Œpre.csv
            },
            "config": {
                "pattern": "auto_pump_status.json",
                "description": "é…ç½®çŠ¶æ€æ–‡ä»¶",
                "max_files": 1
            }
        }
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.base_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        for file_type in self.file_types.keys():
            (self.base_dir / file_type).mkdir(exist_ok=True)
    
    def migrate_existing_files(self):
        """è¿ç§»ç°æœ‰çš„dataå’Œokx_logç›®å½•ä¸­çš„æ–‡ä»¶"""
        print("ğŸš€ å¼€å§‹è¿ç§»ç°æœ‰æ–‡ä»¶åˆ°ç»Ÿä¸€å­˜å‚¨ç›®å½•...")
        
        migration_map = [
            # (æºç›®å½•, ç›®æ ‡å­ç›®å½•, æ–‡ä»¶æ¨¡å¼)
            ("data", "csv_data", "*.csv"),
            ("data", "config", "auto_pump_status.json"),
            ("data/jupiter", "jupiter", "jupiter_tokens_*.json"),
            ("okx_log", "analysis", "analysis_*.json"),
            ("okx_log", "holders", "holders_raw_*.json"),
            ("okx_log", "logs", "okx_crawler_*.log"),
        ]
        
        migrated_count = 0
        for source_dir, target_subdir, pattern in migration_map:
            source_path = Path(source_dir)
            target_path = self.base_dir / target_subdir
            
            if source_path.exists():
                for file_path in source_path.glob(pattern):
                    if file_path.is_file():
                        target_file = target_path / file_path.name
                        try:
                            shutil.move(str(file_path), str(target_file))
                            print(f"  âœ… è¿ç§»: {file_path} -> {target_file}")
                            migrated_count += 1
                        except Exception as e:
                            print(f"  âŒ è¿ç§»å¤±è´¥: {file_path} - {e}")
        
        print(f"ğŸ“¦ è¿ç§»å®Œæˆï¼Œå…±è¿ç§» {migrated_count} ä¸ªæ–‡ä»¶")
        
        # æ¸…ç†ç©ºç›®å½•
        self._cleanup_empty_dirs()
    
    def _cleanup_empty_dirs(self):
        """æ¸…ç†ç©ºç›®å½•"""
        dirs_to_check = ["data/jupiter", "data", "okx_log"]
        for dir_path in dirs_to_check:
            path = Path(dir_path)
            if path.exists() and path.is_dir():
                try:
                    if not any(path.iterdir()):  # ç›®å½•ä¸ºç©º
                        path.rmdir()
                        print(f"  ğŸ—‘ï¸ æ¸…ç†ç©ºç›®å½•: {dir_path}")
                except Exception as e:
                    print(f"  âš ï¸ æ— æ³•æ¸…ç†ç›®å½• {dir_path}: {e}")
    
    def get_file_path(self, file_type: str, filename: str) -> Path:
        """
        è·å–æŒ‡å®šç±»å‹æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        
        Args:
            file_type: æ–‡ä»¶ç±»å‹ (analysis, holders, jupiter, logs, csv_data, config)
            filename: æ–‡ä»¶å
            
        Returns:
            Path: æ–‡ä»¶å®Œæ•´è·¯å¾„
        """
        if file_type not in self.file_types:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
        
        return self.base_dir / file_type / filename
    
    def cleanup_old_files(self, file_type: str = None):
        """
        æ¸…ç†æ—§æ–‡ä»¶
        
        Args:
            file_type: æŒ‡å®šæ¸…ç†çš„æ–‡ä»¶ç±»å‹ï¼ŒNoneè¡¨ç¤ºæ¸…ç†æ‰€æœ‰ç±»å‹
        """
        if file_type:
            if file_type not in self.file_types:
                print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
                return
            self._cleanup_by_type(file_type)
        else:
            for ftype in self.file_types.keys():
                self._cleanup_by_type(ftype)
    
    def _cleanup_by_type(self, file_type: str):
        """æŒ‰ç±»å‹æ¸…ç†æ–‡ä»¶"""
        config = self.file_types[file_type]
        type_dir = self.base_dir / file_type
        pattern = config["pattern"]
        max_files = config["max_files"]
        
        if not type_dir.exists():
            return
        
        # è·å–æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
        files = list(type_dir.glob(pattern))
        if len(files) <= max_files:
            return
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ—§çš„åœ¨å‰
        files.sort(key=lambda x: x.stat().st_mtime)
        
        # è®¡ç®—éœ€è¦åˆ é™¤çš„æ–‡ä»¶æ•°é‡
        files_to_delete = files[:len(files) - max_files]
        
        print(f"ğŸ—‘ï¸ æ¸…ç† {config['description']} æ–‡ä»¶:")
        print(f"   æ€»æ–‡ä»¶æ•°: {len(files)}, é™åˆ¶: {max_files}, å°†åˆ é™¤: {len(files_to_delete)}")
        
        for file_path in files_to_delete:
            try:
                file_path.unlink()
                print(f"   âœ… åˆ é™¤: {file_path.name}")
            except Exception as e:
                print(f"   âŒ åˆ é™¤å¤±è´¥: {file_path.name} - {e}")
    
    def get_storage_info(self) -> Dict:
        """è·å–å­˜å‚¨ä¿¡æ¯"""
        info = {
            "total_files": 0,
            "total_size_mb": 0,
            "by_type": {}
        }
        
        for file_type, config in self.file_types.items():
            type_dir = self.base_dir / file_type
            if not type_dir.exists():
                continue
            
            files = list(type_dir.glob(config["pattern"]))
            total_size = sum(f.stat().st_size for f in files if f.is_file())
            
            info["by_type"][file_type] = {
                "count": len(files),
                "size_mb": round(total_size / 1024 / 1024, 2),
                "description": config["description"],
                "max_files": config["max_files"]
            }
            
            info["total_files"] += len(files)
            info["total_size_mb"] += total_size
        
        info["total_size_mb"] = round(info["total_size_mb"] / 1024 / 1024, 2)
        return info
    
    def auto_cleanup_check(self):
        """è‡ªåŠ¨æ£€æŸ¥å¹¶æ¸…ç†æ–‡ä»¶"""
        total_files = sum(
            len(list((self.base_dir / file_type).glob(config["pattern"])))
            for file_type, config in self.file_types.items()
            if (self.base_dir / file_type).exists()
        )
        
        if total_files > self.max_files:
            print(f"ğŸ“Š å­˜å‚¨ç©ºé—´æ£€æŸ¥: å½“å‰ {total_files} ä¸ªæ–‡ä»¶ï¼Œè¶…è¿‡é™åˆ¶ {self.max_files}")
            self.cleanup_old_files()
        else:
            print(f"ğŸ“Š å­˜å‚¨ç©ºé—´æ£€æŸ¥: å½“å‰ {total_files} ä¸ªæ–‡ä»¶ï¼Œæœªè¶…è¿‡é™åˆ¶ {self.max_files}")


# å…¨å±€æ•°æ®ç®¡ç†å™¨å®ä¾‹
data_manager = DataManager()


def get_data_manager() -> DataManager:
    """è·å–æ•°æ®ç®¡ç†å™¨å®ä¾‹"""
    return data_manager


def migrate_to_unified_storage():
    """è¿ç§»åˆ°ç»Ÿä¸€å­˜å‚¨ç›®å½•"""
    data_manager.migrate_existing_files()


def cleanup_storage():
    """æ¸…ç†å­˜å‚¨ç©ºé—´"""
    data_manager.cleanup_old_files()


def get_storage_status():
    """è·å–å­˜å‚¨çŠ¶æ€"""
    return data_manager.get_storage_info()
